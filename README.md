# llm_portfolio
The primary objective of this project is to show the ability to fine-tune to enhance the model performance on a specific task. In this task, a Large language model has been fine-tuned for code generation. The language of the generated code being Python. The model has been trained on data chosen from a benchmark dataset and its performance has been evaluated on test data using the selected evaluation metric. In the next step, an existing large language model has been used in the data collection process to generate synthetic data. This data has then been used to fine-tune the chosen base model to enhance its performance. In the final step of the project, the training data and synthetic data have been combined and used to further fine-tune the base model to increase its performance. A visualization has been provided to compare the model performances using the selected evaluation metric.
The various fine-tuned models have been assigned the following names:
• Model A: The chosen base model for this project. (not fine-tuned)
• Model B: The fine-tuned model A on the training data of the chosen benchmark dataset.
• Model C: The fine-tuned model A on the generated synthetic data.
• Model D: The fine-tuned model A on the combined data. (training data + synthetic data)

The entire portfolio has been done on Google Colab with the free tier of GPU resources available. 

• Fine-tuning approach: For this task, the PEFT [4], that is Parameter-Efficient Fine- tuning approach was chosen. This method helps in reducing RAM and storage require- ments to a considerable extent. Considering the compute resources for this project, it is important to incorporate this method of fine-tuning. In this approach, the parameters of the chosen model are frozen and only a small number of additional parameters are fine-tuned. This helps in reducing the overall memory footprint. Moreover, since the base model’s parameters are not adjusted, its pre-training knowledge is retained, thereby pre- venting catastrophic forgetting. In this project, the LoRa method of PEFT was used. It is a Low-Rank Adaption of Large Language Models. In this approach, values are added to the parameters in the newly added layers which mitigates the problem of latency during inference time. In addition to implementing the discussed approach, quantization was also incorporated to reduce the memory usage as this project had to be done in Google Colab with limited GPU. That resulted in implementing QLoRa [5]. It applies quanti- zation to the LoRa method. A 4-bit quantization, a double quantization which further reduces the memory footprint are applied in this approach. To implement QLoRa, the pre-trained model has to be loaded in 4-bit quantization along-with defining a LoRaConfig file. This requires using BitsandBytes config. These techniques resulted in a "lighter" and less expensive training of the model.

• Train-test split: It is important to decide the right number of examples for training, validation and testing data to fine-tune and evaluate the model. In general cases, the split for training-validation-testing data is 60-80/10-20/10-20. For training the base model, a training data of 300 examples and validation data of 100 examples were used. The testing data had 100 examples. Since model A also had to be fine-tuned on synthetic data with a size threefold to the training data, and further fine-tuned on the combined training and synthetic data, it was important to choose a suitable size of training data which could be used, keeping in mind the limited computational resources available for this project. Hence the mentioned split for training-testing-validation was used.

• Designing prompt: For generating synthetic data that was used to train the base model A (resulting in model C), designing a proper prompt was very important. While designing the prompt, many things had to be incorporated. Firstly, the prompt was designed with clear instructions, such that when passed to the LLM, the response contained data in the format similar to the chosen benchmark dataset [6]. This is because the generated data will be easy to use for fine-tuning with not much pre-processing needed. For this, I provided the format in which I required the LLM to generate responses. Secondly, the prompt was designed in a way that the LLM generates only the ’Instruction-Output’ pairs without any additional explanation or information. To achieve this I added ’Constraints’ [7] header in my prompt which mentioned what the LLM should not generate, like code explanation or any extra information, other than the expected response. Thirdly, I added two examples in the exact format as I expect the LLM to respond to my query. This helped the LLM to understand how to generate responses. In addition, I also gave a scenario to the LLM to better explain what kind of response it has to generate. I used delimiters to clearly indicate distinct parts of the input. Incorporating these design techniques led to formulating a prompt which when queried to the LLM, gave clean and desired responses, without any additional information.

• Evaluation metric: In order to evaluate the models’ performance, it is essential to choose a suitable evaluation metric. Unlike for machine translation tasks where metrics like BLEU or Rouge are used, for code synthesis evaluation they might not always be the best choice. This is because code and natural language have natural differences. For instance, code has special keywords like if,else,for just to name a few and capturing their importance in the code is not very properly done by the BLEU metric. BLEU also does not capture the structural properties of code. Code is unambiguous and deterministic and to correctly evaluate the correctness of the generated code in this project, I have used the codeBLEU [8] evaluation metric which incorporates the properties of BLEU metric along-with capturing the syntax and semantics of code which are very essential to be taken into account during code evaluation. The codeBLEU score calculates the scores of standard BLEU n-grams, weighted n-grams (for evaluating the keywords present in code), syntactic match of the code and semantic match (for the data-flow in code). Weighted sum of these four components result in the final codeBLEU score. The weights can be adjusted as per requirement. I have kept the weights for each component equal to 0.25 for this project, giving similar importance to all components. Preserving semantic similarity of code is crucial even if the surface level similarity is low. CodeBLEU considers syntax and semantics alongside token similarity. This holistic approach provides a more reliable evaluation for code synthesis tasks.
.
.
.

The following section explains the implementation of the project.

After choosing the model, dataset and all the other design aspects like fine-tuning approach, test-train split, prompt and evaluation metric, it was time to implement them using code. For this, a Google Colab notebook was used. In this section, I will explain how the implementation was done. For ease of understanding, I have structured the Implementation phase using four sections. Each section elaborately explains the steps performed to implement the solution.

• Section 1: In this section, the dependencies for performing the task were installed. There- after, the dataset, the model and its tokenizer were loaded. Since, QLoRa was decided to be implemented, the pre-trained base model was loaded in 4-bit quantization along-with double quantization, using a defined LoraConfig file. The dataset was then split into the training, validation and testing split of 300, 100 and 100 examples. The training data and validation data only had the column ’Prompt’ and rest other columns were removed as they were not necessary for fine-tuning the model. First, inference was taken on the base model A on the test data and its performance was evaluated using the computed code- BLEU score. In the next step, the model A was prepared for training on the chosen train data. For this, the PEFT setup was done as some pre-processing was necessary to prepare it for training. The LoraConfig file was defined and the training hyperparameters were defined in the ’TrainingArguments’ file. Among the many hyperparameters, some were epochs=2, batch size=2, learning rate=1e-4 and optimizer=paged adamw optimizer(32 bit). The model, dataset, LoraConfig and TrainingArguments file were then passed to the SFTTrainer (Supervised Fine-tuning Trainer) that was used for fine-tuning the base model A. After fine-tuning, model B was saved and uploaded in drive to be used for inference. The performance of this fine-tuned model was evaluated on test data using codeBLEU metric.

• Section 2: This section explains the process of implementing the designed prompt to generate synthetic data using the provided API key and model. A function was defined which took the prompt, api token, temperature and top-p values as inputs and in return generated a response/data. I set the temperature in the range of 0.5 - 0.7 so that results were diverse and creative. A higher temperature will result in more creative and random outputs. A loop was used to generate synthetic data by calling the function and passing the prompt. A total of around 1200 data rows were generated, out of which there were some repetitions. The data was then cleaned and the final data was in the format and size similar to the train data, having 903 examples. This data was used to fine-tune model A, resulting in model C. Similar to how model B was trained, LoraConfig, TrainingArguments file were defined and they were passed to the SFTTrainer along-with the model and dataset for training. It was also trained for 2 epochs. Post fine-tuning, model was saved in drive and its performance was captured using codeBLEU metric on the test data.

• Section 3: After fine-tuning the base model A on the training data (resulting in model B) and synthetic data (resulting in model C), both the datasets were combined. This combined dataset was shuffled with seed and used for further fine-tuning the base model A. The combined dataset had 1203 examples and this was used to train model A for 2 epochs. The resulting model D was then saved in drive and used for inference using the codeBLEU metric on the test data.

• Section 4: After fine-tuning model A on various datasets and obtaining four models in the process, the models’ performances were presented in the form of a visualization. A grouped bar chart was plotted using matplotlib library, on the codeBLEU scores obtained during inference time for all four models. The next section discusses about the results obtained and the thoughts around it.
.
.
.

• Results & Discussion: The figure below shows a comparison of the performance of all four models with respect to the codeBLEU score and individual component metrics namely, standard BLEU n-grams, weighted n-grams (for evaluating the keywords present in code), syntactic match of the code and semantic match (for the data-flow in code) that are considered while computing the final codeBLEU score. It is observed that model A has the poorest performance in all four components of the codeBLEU metric including the cumulative codeBLEU score. Model D on the other hand has the best performance in all five metrics. Intuitively, it is expected that since model A has not been fine-tuned specifically on code generation tasks, its performance will be the lowest. Model B which was fine-tuned on 300 training data from the benchmark data has been observed to have an increase in performance throughout all metrics as compared to model A. This is due to the reason that model B has been fine-tuned on a fraction of the Python code generation dataset. The next model C was fine-tuned on 903 examples, threefold the training data of model B. So, intuitively it should have a better performance across the metrics compared to model B, but that is not observed in this task. The reason can be the quality of the synthetic data is not as diverse and comprehensive as compared to the subset of train data in the benchmark dataset that was used for fine-tuning model B. Even-though the number of samples in the synthetic data are more, the quality of the data was not enough to increase the performance of model C. I think, if better quality samples could be generated, model C’s performance would have surpassed model B. Another thing to note is, even-though model C performed poorly as compared to model B, the difference in performance across all metrics is not drastic. There is only a slight difference in performance between the two models. As for model D, it has been fine-tuned on both the training data and the synthetic data. This dataset is both qualitatively and quantitatively richer than the dataset used for fine-tuning all the previous models, hence it was expected to have the best performance across all models. The results of model D validate this intuition. Another interesting aspect has been observed, that across all metrics for all models, the Syntax and Data-flow match scores were higher as compared to the N-grams and weighted N-grams match score. This proves that in code generation/synthesis tasks, syntax and semantics (data-flow) play a crucial role in evaluating model performance rather than simple N-gram word match. It also proves the effectiveness of using codeBLEU as a holistic metric for evaluating model performances in this project.



<img width="772" alt="Screenshot 2024-05-12 at 15 53 56" src="https://github.com/asmita7798/llm_portfolio/assets/166308265/fa0379cc-324a-44d5-98ea-ed241709d21a">


